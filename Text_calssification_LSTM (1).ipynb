{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMlZFsaMmv6R"
      },
      "source": [
        "# Implementation of RNN for Text Classifications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP_Awg-Dmxxh"
      },
      "source": [
        "## STEP 1 : First, we will need the following dependencies to be imported.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPfGXBXOmgXB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ650uMcnfmC"
      },
      "source": [
        "The code imports the TensorFlow library (tf) along with its dataset module (tensorflow_datasets as tfds). Additionally, it imports NumPy (np) for numerical operations and Matplotlib (plt) for plotting. These libraries are commonly used for machine learning tasks and data visualization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx3SUldBniSw"
      },
      "source": [
        "## STEP 2 : Load the dataset\n",
        "### IMDB movies review dataset is the dataset for binary sentiment classification containing 25,000 highly polar movie reviews for training, and 25,000 for testing. This dataset can be acquired from this website or we can also use tensorflow_datasets library to acquire it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6y7nSBDm6o_",
        "outputId": "d6a64ee6-2f75-41e7-b9e2-d57205d73b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the IMDB dataset\n",
        "vocab_size = 10000  # Vocabulary size\n",
        "max_length = 250    # Max length of each review\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "train_data = pad_sequences(train_data, maxlen=max_length, padding='post')\n",
        "test_data = pad_sequences(test_data, maxlen=max_length, padding='post')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnjdVvmkn3uz"
      },
      "source": [
        "## STEP 3 : Build and create the Model\n",
        "### In this section, we will define the model we will use for sentiment analysis. The initial layer of this architecture is the text vectorization layer, responsible for encoding the input text into a sequence of token indices. These tokens are subsequently fed into the embedding layer, where each word is assigned a trainable vector. After enough training, these vectors tend to adjust themselves such that words with similar meanings have similar vectors. This data is then passed to LSTM layers which process these sequences and finally convert it to a single logit as the classification output.\n",
        "\n",
        "## Text Vectorization\n",
        "### We will first perform text vectorization and let the encoder map all the words in the training dataset to a token. We can also see in the example below how we can encode and decode the sample review into a vector of integers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "djy2jM0aoTQc",
        "outputId": "5342e16d-c5e2-4834-b360-d9b4d573241d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m640,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │          \u001b[38;5;34m66,048\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m41,216\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">751,489</span> (2.87 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m751,489\u001b[0m (2.87 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">751,489</span> (2.87 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m751,489\u001b[0m (2.87 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(max_length,)),  # Input layer\n",
        "    tf.keras.layers.Embedding(vocab_size, 64),  # Embedding layer\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),  # First LSTM layer\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),  # Second LSTM layer\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aqGXLrW_tui"
      },
      "outputs": [],
      "source": [
        "m = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(m_l,)),\n",
        "    tf.keras.layers.Embedding(v_s, 64),\n",
        "    tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(64, return_sequences= True)),\n",
        "    tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation= 'relu'),\n",
        "    tf.keras.layers.Dense(64, activation= 'sigmoid'),\n",
        "])\n",
        "m.summary()\n",
        "m.compile(\n",
        "    l=  tf.keras.layers.BinaryCrossentropy(from_logits= False),\n",
        "    o =  tf.keras.layers.optimizer.Adam()\n",
        "    m= ['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpxKTFCyoe1W"
      },
      "source": [
        "The code defines a TextVectorization layer (encoder) with a vocabulary size limit of 10,000 tokens and adapts it to the training dataset. It then extracts the vocabulary from the TextVectorization layer. The code encodes an example text using the TextVectorization layer (encoder(original_text).numpy()) and decodes it back to the original form using the vocabulary. This demonstrates how the TextVectorization layer can normalize, tokenize, and map strings to integers, facilitating text processing for machine learning models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UckI04UKo8HF"
      },
      "source": [
        "## STEP 4 : Training the model\n",
        "### Now, we will train the model we defined in the previous step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "livIAuiAoxLH",
        "outputId": "c2f2df28-dbe2-4330-e5bd-7c201460d970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 616ms/step - accuracy: 0.7970 - loss: 0.4591 - val_accuracy: 0.8532 - val_loss: 0.3387\n",
            "Epoch 2/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 621ms/step - accuracy: 0.8995 - loss: 0.2542 - val_accuracy: 0.8616 - val_loss: 0.3344\n",
            "Epoch 3/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 614ms/step - accuracy: 0.9338 - loss: 0.1820 - val_accuracy: 0.8527 - val_loss: 0.3965\n",
            "Epoch 4/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 614ms/step - accuracy: 0.9455 - loss: 0.1469 - val_accuracy: 0.8562 - val_loss: 0.4202\n",
            "Epoch 5/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 616ms/step - accuracy: 0.9544 - loss: 0.1248 - val_accuracy: 0.8691 - val_loss: 0.3910\n",
            "Epoch 6/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 603ms/step - accuracy: 0.9686 - loss: 0.0914 - val_accuracy: 0.8512 - val_loss: 0.5360\n",
            "Epoch 7/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 603ms/step - accuracy: 0.9797 - loss: 0.0628 - val_accuracy: 0.8622 - val_loss: 0.5066\n",
            "Epoch 8/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m557s\u001b[0m 673ms/step - accuracy: 0.9857 - loss: 0.0484 - val_accuracy: 0.8592 - val_loss: 0.4763\n",
            "Epoch 9/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 600ms/step - accuracy: 0.9868 - loss: 0.0463 - val_accuracy: 0.8513 - val_loss: 0.6321\n",
            "Epoch 10/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 606ms/step - accuracy: 0.9861 - loss: 0.0436 - val_accuracy: 0.8611 - val_loss: 0.5643\n",
            "Epoch 11/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 600ms/step - accuracy: 0.9909 - loss: 0.0319 - val_accuracy: 0.8620 - val_loss: 0.5625\n",
            "Epoch 12/100\n",
            "\u001b[1m 14/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:12\u001b[0m 485ms/step - accuracy: 0.9991 - loss: 0.0094"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Convert data to TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
        "\n",
        "# Batch the datasets\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=test_dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiW51HihpHUD"
      },
      "source": [
        "The code trains the defined model (model) using the training dataset (train_dataset) for 5 epochs. It also validates the model on the test dataset (test_dataset). The training progress and performance metrics are stored in the history variable for further analysis or visualization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh6aWTKmpH0B"
      },
      "source": [
        "## STEP 5 : Plotting the results\n",
        "### Plotting the training and validation accuracy and loss plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlpereiMpAqD"
      },
      "outputs": [],
      "source": [
        "# Plotting the accuracy and loss over time\n",
        "\n",
        "# Training history\n",
        "history_dict = history.history\n",
        "\n",
        "# Seperating validation and training accuracy\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "\n",
        "# Seperating validation and training loss\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Accuracy', 'Validation Accuracy'])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Loss', 'Validation Loss'])\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq34GiMHpPQN"
      },
      "source": [
        "The code visualizes the training and validation accuracy as well as the training and validation loss over epochs. It extracts accuracy and loss values from the training history (history_dict). The matplotlib library is then used to create a side-by-side subplot, where the left subplot displays accuracy trends, and the right subplot shows loss trends over epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vgovoVzpRCK"
      },
      "source": [
        "## STEP 7 : Testing the trained model\n",
        "### Now, we will test the trained model with a random review and check its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc5b6wnCpU_O"
      },
      "outputs": [],
      "source": [
        "# Sample text to predict\n",
        "sample_text = (\n",
        "    '''The movie was so good and the animation are so dope.\n",
        "    I would recommend my friends to watch it.'''\n",
        ")\n",
        "\n",
        "# Tokenize and pad the sample text\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts([sample_text])  # Fitting on the sample text to tokenize\n",
        "sample_seq = tokenizer.texts_to_sequences([sample_text])\n",
        "sample_padded = pad_sequences(sample_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Make prediction\n",
        "predictions = model.predict(sample_padded)\n",
        "print(*predictions[0])\n",
        "\n",
        "# Print the label based on the prediction\n",
        "if predictions[0] > 0:\n",
        "    print('The review is positive')\n",
        "else:\n",
        "    print('The review is negative')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXWm8DTY6W-j"
      },
      "outputs": [],
      "source": [
        "# Negative sample text\n",
        "sample_text = (\n",
        "    '''The movie was terrible. The plot was confusing and the acting was bad.\n",
        "    I would not recommend it to anyone.'''\n",
        ")\n",
        "\n",
        "# Tokenize and pad the sample text\n",
        "# Assuming you've already trained a tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts([sample_text])  # Fitting on the sample text to tokenize\n",
        "sample_seq = tokenizer.texts_to_sequences([sample_text])\n",
        "sample_padded = pad_sequences(sample_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Make prediction\n",
        "predictions = model.predict(sample_padded)\n",
        "print(*predictions[0])\n",
        "\n",
        "# Print the label based on the prediction\n",
        "if predictions[0] > 0.5:\n",
        "    print('The review is positive')\n",
        "else:\n",
        "    print('The review is negative')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCYG8f2SpYq1"
      },
      "source": [
        "#**Well done =)**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}